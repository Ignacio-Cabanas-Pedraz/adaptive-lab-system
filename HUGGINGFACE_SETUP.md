# HuggingFace Setup for Llama 3 Access

The system uses Meta's Llama 3 8B model for AI-powered template validation. This model is **gated** on HuggingFace and requires approval.

## Why You Need This

Llama 3 enhances template generation by:
- ‚úÖ Validating extracted parameters (catches regex errors)
- ‚úÖ Identifying chemicals and equipment automatically
- ‚úÖ Generating safety notes
- ‚úÖ Improving overall accuracy from ~75% to ~92%

**Without Llama**: The system still works using regex-only extraction, but with lower accuracy.

## Setup Steps

### 1. Request Model Access (One-time)

1. **Visit**: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
2. **Click**: "Request Access" button
3. **Fill out** the form with:
   - Your name
   - Email
   - Organization (can be "Personal" or "Academic")
   - Intended use (e.g., "Laboratory procedure template generation for research")
4. **Accept** Meta's license agreement
5. **Submit** the request

**Approval time**: Usually 1-24 hours (sometimes instant)

You'll receive an email when approved.

### 2. Create HuggingFace Account Token

1. **Go to**: https://huggingface.co/settings/tokens
2. **Click**: "New token"
3. **Name it**: `lab-system` (or any name you prefer)
4. **Select role**: `Read` (default is fine)
5. **Click**: "Generate"
6. **Copy** the token (starts with `hf_...`)

‚ö†Ô∏è **Save this token** - you won't see it again!

### 3. Authenticate CLI

Open your terminal and run:

```bash
# Install HuggingFace CLI (if not already installed)
pip install huggingface-hub

# Login with your token
huggingface-cli login
```

When prompted:
1. **Paste** your token (it won't show as you type - this is normal)
2. **Press** Enter
3. You should see: `Login successful`

Your token is now saved in `~/.huggingface/token`

### 4. Test Access

Verify you can access the model:

```bash
python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct'); print('‚úÖ Access granted!')"
```

If successful, you'll see: `‚úÖ Access granted!`

If you get a 403 error, wait for approval or check your token.

### 5. Restart Flask Backend

After authentication, restart your Flask server:

```bash
# Stop the server (Ctrl+C)
# Start it again
python app.py
```

The system will now use Llama for validation!

## Troubleshooting

### "403 Client Error: Cannot access gated repo"

**Cause**: Not approved or not logged in

**Solution**:
1. Check email for approval confirmation
2. Verify login: `huggingface-cli whoami`
3. Re-login if needed: `huggingface-cli login`

### "401 Unauthorized"

**Cause**: Invalid or expired token

**Solution**:
1. Generate a new token: https://huggingface.co/settings/tokens
2. Login again: `huggingface-cli login`

### "Still not working after approval"

**Cause**: Token cached before approval

**Solution**:
```bash
# Logout
huggingface-cli logout

# Login again with your token
huggingface-cli login
```

### "Running on CPU instead of GPU"

**Cause**: PyTorch not detecting GPU

**Check**:
```bash
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

If `False`, you're running on CPU (slow but works).

For GPU support, install CUDA-enabled PyTorch.

## Using Without Llama

The system gracefully degrades if Llama is not available:

- ‚úÖ Templates still generated (regex-based)
- ‚úÖ Parameters still extracted (85% accuracy vs 95% with Llama)
- ‚ö†Ô∏è No chemicals/equipment auto-detection
- ‚ö†Ô∏è No safety notes
- ‚ö†Ô∏è No AI validation of parameters

You'll see a warning in the UI: "‚ö†Ô∏è Regex-based extraction only"

To upgrade to full AI validation, follow the steps above.

## Alternative: Use a Different Model (Advanced)

If you can't access Llama 3, you can modify `src/procedure/llm_validator.py` to use:
- Llama 2 (less restricted)
- Mistral 7B (fully open)
- GPT-3.5/4 via OpenAI API (requires API key)

This requires code changes and is not officially supported.

## Cost & Privacy

- ‚úÖ **100% Free** - No API costs
- ‚úÖ **100% Local** - No data sent to external servers
- ‚úÖ **Offline** - Works without internet (after model download)
- üíæ **~9GB VRAM** - Runs on consumer GPUs
- ‚è±Ô∏è **~1-2 sec/step** - Fast enough for real-time use

## Summary

1. Request access: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
2. Get token: https://huggingface.co/settings/tokens
3. Login: `huggingface-cli login`
4. Test: `python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')"`
5. Restart: `python app.py`

That's it! The system will now use AI validation automatically.

---

**Questions?** Check the [main README](README.md) or open an issue on GitHub.
